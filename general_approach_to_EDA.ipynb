{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace=True) \n",
    "''' Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like “Other” might be more sensible'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binning can be applied on both categorical and numerical data:\n",
    "#####  Numerical Binning Example\n",
    "Value      Bin       \n",
    "0-30   ->  Low       \n",
    "31-70  ->  Mid       \n",
    "71-100 ->  High\n",
    "#####  Categorical Binning Example\n",
    "Value      Bin       \n",
    "Spain  ->  Europe      \n",
    "Italy  ->  Europe       \n",
    "Chile  ->  South America\n",
    "Brazil ->  South America\n",
    "\n",
    "Every time you bin something, you sacrifice information and make your data more regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Binning Example\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "'''   value   bin\n",
    "0      2   Low\n",
    "1     45   Mid\n",
    "2      7   Low\n",
    "3     85  High\n",
    "4     28   Low'''\n",
    "#Categorical Binning Example\n",
    "'''     Country\n",
    "0      Spain\n",
    "1      Chile\n",
    "2  Australia\n",
    "3      Italy\n",
    "4     Brazil'''\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n",
    "'''     Country      Continent\n",
    "0      Spain         Europe\n",
    "1      Chile  South America\n",
    "2  Australia          Other\n",
    "3      Italy         Europe\n",
    "4     Brazil  South America'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)\n",
    "'''   value  log(x+1)  log(x-min(x)+1)\n",
    "0      2   1.09861          3.25810\n",
    "1     45   3.82864          4.23411\n",
    "2    -23       nan          0.00000\n",
    "3     85   4.45435          4.69135\n",
    "4     28   3.36730          3.95124\n",
    "5      2   1.09861          3.25810\n",
    "6     35   3.58352          4.07754\n",
    "7    -12       nan          2.48491'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding \n",
    "This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_dummies function of Pandas. This function maps all values in a column to multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "data = data.join(encoded_columns).drop('column', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Categorical Column Grouping  - first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.\n",
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Second option is to make a pivot table. This approach resembles the encoding method in the \n",
    "preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values\n",
    "between grouped and encoded columns. This would be a good \n",
    "option if you aim to go beyond binary flag columns and merge \n",
    "multiple features into aggregated features, which are more informative'''\n",
    "#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Column Grouping\n",
    "use sum and mean functions in most of the cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.name\n",
    "'''0  Luther N. Gonzalez\n",
    "1    Charles M. Young\n",
    "2        Terry Lawson\n",
    "3       Kristen White\n",
    "4      Thomas Logsdon'''\n",
    "#Extracting first names\n",
    "data.name.str.split(\" \").map(lambda x: x[0])\n",
    "'''0     Luther\n",
    "1    Charles\n",
    "2      Terry\n",
    "3    Kristen\n",
    "4     Thomas'''\n",
    "#Extracting last names\n",
    "data.name.str.split(\" \").map(lambda x: x[-1])\n",
    "'''0    Gonzalez\n",
    "1       Young\n",
    "2      Lawson\n",
    "3       White\n",
    "4     Logsdon'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#String extraction example\n",
    "data.title.head()\n",
    "'''0                      Toy Story (1995)\n",
    "1                        Jumanji (1995)\n",
    "2               Grumpier Old Men (1995)\n",
    "3              Waiting to Exhale (1995)\n",
    "4    Father of the Bride Part II (1995)'''\n",
    "data.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]\n",
    "'''0    1995\n",
    "1    1995\n",
    "2    1995\n",
    "3    1995\n",
    "4    1995'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = 'Toy Story (1995)','Jumanji (1995)', 'Grumpier Old Men (1995)', 'Waiting to Exhale (1995)','Father of the Bride Part II (1995']\n",
    "l = 'Toy Story (1995), Jumanji (1995), Grumpier Old Men (1995), Waiting to Exhale (1995),  Father of the Bride Part II (1995)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = l.split(   \"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling      \n",
    "      two common ways of scaling:\n",
    "\n",
    "##### Normalization      between 0 and 1 \n",
    "Xnorm = X - Xmin / Xmax - Xmin\n",
    "##### Standardization (or z-score normalization) scales the values while taking into account standard deviation\n",
    "z = x - mean / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())\n",
    "'''   value  normalized\n",
    "0      2        0.23\n",
    "1     45        0.63\n",
    "2    -23        0.00\n",
    "3     85        1.00\n",
    "4     28        0.47\n",
    "5      2        0.23\n",
    "6     35        0.54\n",
    "7    -12        0.10'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()\n",
    "'''   value  standardized\n",
    "0      2         -0.52\n",
    "1     45          0.70\n",
    "2    -23         -1.23\n",
    "3     85          1.84\n",
    "4     28          0.22\n",
    "5      2         -0.52\n",
    "6     35          0.42\n",
    "7    -12         -0.92'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 5 , 60, 44, 11, -7, -46, 55]\n",
    "def norm(n, l):\n",
    "    lmin = min(l); lmax = max(l); print('Lmin and lmax are: ', lmin, lmax)\n",
    "    up = n - lmin; print('Up: ', up)\n",
    "    down = lmax - lmin; print('down: ', down)\n",
    "    res = up / down; print('res = ', res)\n",
    "    return res\n",
    "j = [norm(n, l) for n in l] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = pd.DataFrame({'date':\n",
    "['01-01-2017',\n",
    "'04-12-2008',\n",
    "'23-06-1988',\n",
    "'25-08-1999',\n",
    "'20-02-1993',\n",
    "]})\n",
    "\n",
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()\n",
    "\n",
    "'''   date    year  month  passed_years  passed_months   day_name\n",
    "0 2017-01-01  2017      1             2             26     Sunday\n",
    "1 2008-12-04  2008     12            11            123   Thursday\n",
    "2 1988-06-23  1988      6            31            369   Thursday\n",
    "3 1999-08-25  1999      8            20            235  Wednesday\n",
    "4 1993-02-20  1993      2            26            313   Saturday'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "972343a0b0a3a4c738602a88ea5a445a822fb27b40865c29837d78eba7fe57e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
